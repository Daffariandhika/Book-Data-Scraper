# Book-Data-Scraper
 A Python-based web scraping tool designed to batch collect and analyze book data from Goodreads.,

## **Overview**

This utility includes two Python scripts designed for scraping book data from GoodReads:

- **`url_scraper.py`**: Collects book URLs from a specific genre or search query on GoodReads.
- **`books_scraper.py`**: Extracts detailed information about books from the collected URLs.
- The data is saved in both **JSON** and **CSV** formats.
---
## **Features**

### `url_scraper.py`
- Scrapes book URLs from a specific genre or search query on GoodReads.
- Automatically handles pagination and extracts up to `max_urls` book URLs.
- Saves the collected URLs in a Python file for further scraping.

### `books_scraper.py`
- Extracts detailed information about each book.
- Handles retries for failed requests.
- Saves the scraped data in **JSON** and **CSV** formats.
---
## **Prerequisites**

Ensure you have **Python 3.10** or higher installed. Additionally, you'll need the following Python libraries:

```bash
pip install pandas requests beautifulsoup4
```
---
## **Scripts Overview**

- ## `url_scraper.py` 
This script collects book URLs from a specified genre or search result on Goodreads. You can adjust the `max_urls` parameter to specify how many URLs you want to collect.

### **How to Use:**
Modify the `search_url` variable to specify the genre or search query.
Run the script, which will scrape URLs from the first page and continue scraping pages until the `max_urls` limit is reached.
The URLs will be saved in a Python file **`books_url.py`** for further scraping.

- ## `books_scraper.py`
This script uses the URLs from the **`books_url.py`** to scrape detailed book information. The data will be saved in both **JSON** and **CSV** formats.

### **How to Use:**
Ensure that the file **`books_url.py`** (generated by the first script) is in the same directory.
Run the script, which will scrape detailed data for each book and save it into two files: **`books.json`** and **`books.csv`**.

---

## **Parameters & Configuration**

## `url_scraper.py`
- `genre_url`: URL to scrape.
- `max_urls`: Number of URLs to scrape.
- `delay`: Delay in seconds between requests to avoid overloading the server.

## `books_scraper.py`
- *Rate Limiting:* Automatically includes rate-limiting to avoid being blocked.
- *Retry Logic:* The script retries up to 3 times for failed requests.
- *Saved Files:* Scraped data is saved in both **JSON** and **CSV** formats **`books.json`** and **`books.csv`**.

---

## **Troubleshooting**

- *Invalid URL or Request Issues:* Ensure the provided Goodreads URL is correct. If the script fails to fetch data, it will retry multiple times.
- *No Data Scraped:* Ensure you have collected a sufficient number of URLs using **`url_scraper.py`** The second script needs these URLs to scrape book data.
